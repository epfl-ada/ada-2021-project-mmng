{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "import sys\n",
    "\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "# import scipy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing without trunctions\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO describe a bit about party labeling and how we only keep politicians!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting which political party does a quote's message lean towards\n",
    "\n",
    "Our first task is to create a model that can classify the political inclinations of a single quote.\n",
    "\n",
    "Since, our classification task strongly resembles that of NLP sentiment analysis\n",
    "we applied a corresponding methodology.\n",
    "\n",
    "Which can be summed up by the following steps by the following steps:\n",
    "1. Label data (done in part2)\n",
    "2. Clean quotations (augmented in part3, this part)\n",
    "3. Vectorize quotations\n",
    "4. Train models and select optimal model for prediction\n",
    "\n",
    "Due to the complex nature of the task, which is to predict whether a single quote\n",
    "was said by a republican or democrat politician reaching a high accuracy is very\n",
    "difficult and so we had to optimize all the steps described above.\n",
    "\n",
    "Also, as noted in the course and on various online resources. It is sometimes\n",
    "better to have less data cleaning and text preprocessing in an\n",
    "NLP sentiment analysis tasks.\n",
    "\n",
    "We therfore had to find the optimal pipeline. This required finding the best \n",
    "combination of text preprocessor/cleaner, vectorizer and ML model. Given, that\n",
    "this isn't an ML class we tested a few computationally simple models\n",
    "(which can also be trained in a reasonable time frame) and focused rather on\n",
    "optimizing the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal level of preprocessing\n",
    "\n",
    "Our strategy in order to complete this task is to generate a dataset of quotes\n",
    "where each quote has with 5 different levels of text preprocessing, ranging \n",
    "from very light to very strong. Then we run cross validation with a few simple models \n",
    "(to keep execution time reasonable) and aggregate a few performance metrics to\n",
    "identify which level of preprocessing yielded the best performance with our model.\n",
    "\n",
    "Unlike for the final classification pipeline/model we perform all this only with \n",
    "quotes from 2020 since it is a reasonably sized snapshot of the data for this task.\n",
    "\n",
    "Each level of preprocessing was given a 1 letter name A,B,...,E.\n",
    "Here is a description of the 5 different levels of preprocessing:\n",
    "- A: Some trivial cleanup, removing digits and diacritics.\n",
    "- B: All steps in A + casefolding and removing punctuation.\n",
    "- C: All steps in B + removing stopwords.\n",
    "- D: All steps in C + stemming. Using the snowball stemmer.\n",
    "- E: All steps in C + lemmatization. Using nltk's WordNetLemmatizer.\n",
    "\n",
    "We perform the analysis in the cells below. We first prepare the data\n",
    "before running out tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396818, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>party_label</th>\n",
       "      <th>quotation_cleanA</th>\n",
       "      <th>quotation_cleanB</th>\n",
       "      <th>quotation_cleanC</th>\n",
       "      <th>quotation_cleanD</th>\n",
       "      <th>quotation_cleanE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-16-000088</td>\n",
       "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
       "      <td>Sue Myrick</td>\n",
       "      <td>2020-01-16 12:00:13</td>\n",
       "      <td>Q367796</td>\n",
       "      <td>R</td>\n",
       "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
       "      <td>department of homeland security was livid and ...</td>\n",
       "      <td>department homeland security livid strongly ur...</td>\n",
       "      <td>depart homeland secur livid strong urg agenda ...</td>\n",
       "      <td>department homeland security livid strongly ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-19-000276</td>\n",
       "      <td>[ These ] actions will allow households who ha...</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>2020-03-19 19:14:00</td>\n",
       "      <td>Q816459</td>\n",
       "      <td>R</td>\n",
       "      <td>[ These ] actions will allow households who ha...</td>\n",
       "      <td>these actions will allow households who have a...</td>\n",
       "      <td>actions allow households fha insured mortgage ...</td>\n",
       "      <td>action allow household fha insur mortgag meet ...</td>\n",
       "      <td>action allow household fha insured mortgage me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-22-009723</td>\n",
       "      <td>be pivotal in addressing financial frustrations</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>2020-01-22 21:07:39</td>\n",
       "      <td>Q816459</td>\n",
       "      <td>R</td>\n",
       "      <td>be pivotal in addressing financial frustrations</td>\n",
       "      <td>be pivotal in addressing financial frustrations</td>\n",
       "      <td>pivotal addressing financial frustrations</td>\n",
       "      <td>pivot address financi frustrat</td>\n",
       "      <td>pivotal addressing financial frustration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-04-110477</td>\n",
       "      <td>We're talking about `Do we want to continue th...</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>2020-02-04 23:02:36</td>\n",
       "      <td>Q816459</td>\n",
       "      <td>R</td>\n",
       "      <td>We're talking about `Do we want to continue th...</td>\n",
       "      <td>we re talking about do we want to continue the...</td>\n",
       "      <td>talking want continue lifestyle characterized ...</td>\n",
       "      <td>talk want continu lifestyl character american ...</td>\n",
       "      <td>talking want continue lifestyle characterized ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-28-051506</td>\n",
       "      <td>It's not just a matter of throwing more and mo...</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>2020-01-28 19:23:36</td>\n",
       "      <td>Q816459</td>\n",
       "      <td>R</td>\n",
       "      <td>It's not just a matter of throwing more and mo...</td>\n",
       "      <td>it s not just a matter of throwing more and mo...</td>\n",
       "      <td>matter throwing money vouchers services gettin...</td>\n",
       "      <td>matter throw money voucher servic get peopl sy...</td>\n",
       "      <td>matter throwing money voucher service getting ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2020-01-16-000088  [ Department of Homeland Security ] was livid ...   \n",
       "1  2020-03-19-000276  [ These ] actions will allow households who ha...   \n",
       "2  2020-01-22-009723    be pivotal in addressing financial frustrations   \n",
       "3  2020-02-04-110477  We're talking about `Do we want to continue th...   \n",
       "4  2020-01-28-051506  It's not just a matter of throwing more and mo...   \n",
       "\n",
       "      speaker                date       id party_label  \\\n",
       "0  Sue Myrick 2020-01-16 12:00:13  Q367796           R   \n",
       "1  Ben Carson 2020-03-19 19:14:00  Q816459           R   \n",
       "2  Ben Carson 2020-01-22 21:07:39  Q816459           R   \n",
       "3  Ben Carson 2020-02-04 23:02:36  Q816459           R   \n",
       "4  Ben Carson 2020-01-28 19:23:36  Q816459           R   \n",
       "\n",
       "                                    quotation_cleanA  \\\n",
       "0  [ Department of Homeland Security ] was livid ...   \n",
       "1  [ These ] actions will allow households who ha...   \n",
       "2    be pivotal in addressing financial frustrations   \n",
       "3  We're talking about `Do we want to continue th...   \n",
       "4  It's not just a matter of throwing more and mo...   \n",
       "\n",
       "                                    quotation_cleanB  \\\n",
       "0  department of homeland security was livid and ...   \n",
       "1  these actions will allow households who have a...   \n",
       "2    be pivotal in addressing financial frustrations   \n",
       "3  we re talking about do we want to continue the...   \n",
       "4  it s not just a matter of throwing more and mo...   \n",
       "\n",
       "                                    quotation_cleanC  \\\n",
       "0  department homeland security livid strongly ur...   \n",
       "1  actions allow households fha insured mortgage ...   \n",
       "2          pivotal addressing financial frustrations   \n",
       "3  talking want continue lifestyle characterized ...   \n",
       "4  matter throwing money vouchers services gettin...   \n",
       "\n",
       "                                    quotation_cleanD  \\\n",
       "0  depart homeland secur livid strong urg agenda ...   \n",
       "1  action allow household fha insur mortgag meet ...   \n",
       "2                     pivot address financi frustrat   \n",
       "3  talk want continu lifestyl character american ...   \n",
       "4  matter throw money voucher servic get peopl sy...   \n",
       "\n",
       "                                    quotation_cleanE  \n",
       "0  department homeland security livid strongly ur...  \n",
       "1  action allow household fha insured mortgage me...  \n",
       "2           pivotal addressing financial frustration  \n",
       "3  talking want continue lifestyle characterized ...  \n",
       "4  matter throwing money voucher service getting ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load preprocessed data containing all variants of text processing\n",
    "path = fixpath(QUOTES_2020_LABELED_CLEANED_VARIANTS)\n",
    "\n",
    "df_raw = pd.read_json(path, orient='records', lines=True)\n",
    "print(df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# Droping quotes of people in both parties (except most popular members who were labeled manually)\n",
    "df = df[df['party_label'] != 'RD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop very short quotes as a quote that is particularly short will give us\n",
    "little information and would most likely be irrelevant in the classification\n",
    "task. We just drop quotes shorter than 90% of all other quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(349675, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droping short quotes. Quotes shorter than 90% of all other quotes\n",
    "df = drop_short_quotes(df, threshold_quantile=0.1, quote_col_name='quotation_cleanE')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    198353\n",
       "R    151322\n",
       "Name: party_label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if dataset is balanced\n",
    "df.party_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is unbalanced. Since we have a lot of data we just downsample\n",
    "df = downsample(df, 'party_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    151322\n",
       "R    151322\n",
       "Name: party_label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the data is well balanced now\n",
    "df['party_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several different sized version of the data for convenience. Since some\n",
    "# models we test take long to train. Our final prediction for best level of\n",
    "# preprocessing will be done on the full data frame (~220k quotes from 2020)\n",
    "# that we generated above.\n",
    "\n",
    "df_micro = df.sample(1000)\n",
    "df_mini = df.sample(10000)\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifer(df, pipeline, break_after_one_iter=False):\n",
    "    \"\"\"\n",
    "    Function to test different all version of preprocessed quotes with a given\n",
    "    classifer.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'quotation_cleanA',\n",
    "        'quotation_cleanB',\n",
    "        'quotation_cleanC',\n",
    "        'quotation_cleanD',\n",
    "        'quotation_cleanE',\n",
    "    ]\n",
    "\n",
    "    for col in cols:\n",
    "        \n",
    "        # Get quotation preprocessing variant\n",
    "        X = df[col].values\n",
    "\n",
    "        # Get label and convert to useful format\n",
    "        y = df['party_label'].values\n",
    "        y = convert_labels(y)\n",
    "\n",
    "        # Run cross validation with different metrics\n",
    "        # scoring=['accuracy', 'precision', 'recall', 'f1']\n",
    "        scoring=['accuracy', 'f1']\n",
    "        res = cross_validate(pipeline, X, y, scoring=scoring, cv=3)\n",
    "        res.pop('score_time')\n",
    "\n",
    "        # Print results\n",
    "        print(f'Col: {col}')\n",
    "        print_cross_validate_results(res)\n",
    "        \n",
    "        if break_after_one_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each level of preprocessed text we run cross validations with 3 different ML models. \n",
    "Multinomial Naive Bayes, LogisticRegression and Gradient Boosted Trees.\n",
    "\n",
    "We only use Tfidf vectorization but test different levels of N-gram expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 5.078\tstd: 0.628\n",
      "\ttest_accuracy        - \tavg: 0.690\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.694\tstd: 0.001\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 4.875\tstd: 0.612\n",
      "\ttest_accuracy        - \tavg: 0.690\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.694\tstd: 0.001\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 2.725\tstd: 0.088\n",
      "\ttest_accuracy        - \tavg: 0.689\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.692\tstd: 0.001\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 2.699\tstd: 0.092\n",
      "\ttest_accuracy        - \tavg: 0.680\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.683\tstd: 0.000\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 2.971\tstd: 0.288\n",
      "\ttest_accuracy        - \tavg: 0.686\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.689\tstd: 0.001\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 10.666\tstd: 0.746\n",
      "\ttest_accuracy        - \tavg: 0.670\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.666\tstd: 0.000\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 11.970\tstd: 2.888\n",
      "\ttest_accuracy        - \tavg: 0.670\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.666\tstd: 0.000\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 7.622\tstd: 2.836\n",
      "\ttest_accuracy        - \tavg: 0.669\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.665\tstd: 0.000\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 4.686\tstd: 0.382\n",
      "\ttest_accuracy        - \tavg: 0.664\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.659\tstd: 0.000\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 5.684\tstd: 0.295\n",
      "\ttest_accuracy        - \tavg: 0.668\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.663\tstd: 0.000\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('clf', LogisticRegression(max_iter=1000)),\n",
    "        ])\n",
    "\n",
    "# test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 157.447\tstd: 2.448\n",
      "\ttest_accuracy        - \tavg: 0.589\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.494\tstd: 0.003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nf/186rd3kx35v_x13p5402wpsr0000gn/T/ipykernel_39206/2413891448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         ])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_classifer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/nf/186rd3kx35v_x13p5402wpsr0000gn/T/ipykernel_39206/3534891979.py\u001b[0m in \u001b[0;36mtest_classifer\u001b[0;34m(df, pipeline, break_after_one_iter)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# scoring=['accuracy', 'precision', 'recall', 'f1']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score_time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    268\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    269\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    664\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \"\"\"\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('clf', GradientBoostingClassifier()),\n",
    "        ])\n",
    "\n",
    "# Very long to run do sometime later\n",
    "# test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 6.733\tstd: 0.309\n",
      "\ttest_accuracy        - \tavg: 0.674\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.672\tstd: 0.001\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 5.830\tstd: 0.211\n",
      "\ttest_accuracy        - \tavg: 0.674\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.672\tstd: 0.001\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 4.404\tstd: 0.248\n",
      "\ttest_accuracy        - \tavg: 0.672\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.670\tstd: 0.000\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 4.410\tstd: 0.185\n",
      "\ttest_accuracy        - \tavg: 0.666\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.664\tstd: 0.001\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 4.574\tstd: 0.207\n",
      "\ttest_accuracy        - \tavg: 0.669\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.667\tstd: 0.000\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('clf', LinearSVC()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try using using unigrams and bigrams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 17.314\tstd: 0.677\n",
      "\ttest_accuracy        - \tavg: 0.733\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.741\tstd: 0.000\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 14.906\tstd: 0.187\n",
      "\ttest_accuracy        - \tavg: 0.733\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.740\tstd: 0.000\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 11.646\tstd: 0.479\n",
      "\ttest_accuracy        - \tavg: 0.743\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.747\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 10.702\tstd: 0.008\n",
      "\ttest_accuracy        - \tavg: 0.739\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.743\tstd: 0.001\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 11.538\tstd: 0.058\n",
      "\ttest_accuracy        - \tavg: 0.742\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.745\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 28.409\tstd: 1.658\n",
      "\ttest_accuracy        - \tavg: 0.695\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.690\tstd: 0.002\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 29.686\tstd: 5.530\n",
      "\ttest_accuracy        - \tavg: 0.695\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.690\tstd: 0.002\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 40.314\tstd: 4.547\n",
      "\ttest_accuracy        - \tavg: 0.700\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.694\tstd: 0.001\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 36.972\tstd: 4.372\n",
      "\ttest_accuracy        - \tavg: 0.699\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.693\tstd: 0.002\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 43.261\tstd: 2.992\n",
      "\ttest_accuracy        - \tavg: 0.700\tstd: 0.003\n",
      "\ttest_f1              - \tavg: 0.695\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "            ('clf', LogisticRegression(max_iter=1000)),\n",
    "        ])\n",
    "\n",
    "# test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 18.149\tstd: 1.085\n",
      "\ttest_accuracy        - \tavg: 0.708\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.706\tstd: 0.002\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 17.297\tstd: 1.203\n",
      "\ttest_accuracy        - \tavg: 0.708\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.706\tstd: 0.002\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 12.830\tstd: 0.049\n",
      "\ttest_accuracy        - \tavg: 0.718\tstd: 0.003\n",
      "\ttest_f1              - \tavg: 0.716\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 10.893\tstd: 0.191\n",
      "\ttest_accuracy        - \tavg: 0.714\tstd: 0.003\n",
      "\ttest_f1              - \tavg: 0.712\tstd: 0.002\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 11.049\tstd: 0.225\n",
      "\ttest_accuracy        - \tavg: 0.717\tstd: 0.003\n",
      "\ttest_f1              - \tavg: 0.714\tstd: 0.003\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "            ('clf', LinearSVC()),\n",
    "        ])\n",
    "\n",
    "# test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the best performing classifer is Multinomial Naive Bayes, its also\n",
    "the fastest one to train.\n",
    "\n",
    "How about adding trigrams too! We don't train all 3 models as training times\n",
    "get out of hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 34.931\tstd: 0.698\n",
      "\ttest_accuracy        - \tavg: 0.743\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.751\tstd: 0.000\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 36.490\tstd: 1.254\n",
      "\ttest_accuracy        - \tavg: 0.743\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.751\tstd: 0.000\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 23.508\tstd: 0.794\n",
      "\ttest_accuracy        - \tavg: 0.751\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.754\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 23.414\tstd: 1.621\n",
      "\ttest_accuracy        - \tavg: 0.749\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.753\tstd: 0.002\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 22.466\tstd: 1.378\n",
      "\ttest_accuracy        - \tavg: 0.750\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.753\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,3))),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 37.191\tstd: 1.032\n",
      "\ttest_accuracy        - \tavg: 0.718\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.715\tstd: 0.001\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 32.412\tstd: 0.392\n",
      "\ttest_accuracy        - \tavg: 0.719\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.715\tstd: 0.001\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 21.432\tstd: 0.398\n",
      "\ttest_accuracy        - \tavg: 0.724\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.721\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 20.245\tstd: 0.462\n",
      "\ttest_accuracy        - \tavg: 0.723\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.720\tstd: 0.002\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 21.807\tstd: 1.688\n",
      "\ttest_accuracy        - \tavg: 0.724\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.720\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,3))),\n",
    "            ('clf', LinearSVC()),\n",
    "        ])\n",
    "\n",
    "# test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we achieve our best accuracy of 72.4%, with both LinearSVC and MultinomialNB. Eventhough, it is a rather low score we found it to be acceptable given the difficulty of the task. After all predicting predicting the political party of a speaker based on one quote alone is quite a feat. We assume this could be further improved by using word2vec or BERT but that would be beyond the scope of this project.\n",
    "\n",
    "Later, we will also aggregate our predictions to predict the political affiliation of a speaker based on all the quotes that are attributed to them and as such we can expect a better performance there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,4))),\n",
    "            ('clf', LinearSVC()),\n",
    "        ])\n",
    "\n",
    "# test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(2,2))),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6809d3f7e90e77d8ec96c72c39a08b2522313d47c0f1d91518fdbbaf790b394"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ada': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
