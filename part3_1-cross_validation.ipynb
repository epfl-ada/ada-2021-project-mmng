{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "import sys\n",
    "\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "# import scipy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing without trunctions\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements to preprocessing pipeline since part2\n",
    "\n",
    "We implemented multiple improvements to our preprocessing pipeline since the previous milestone in order to improve classification results. Here's a list of issues we noticed and improvements to address them:\n",
    "\n",
    "1. In order to select relevant quotes we used the occupation attribute to select only speakers who are politicians. As quotes by actors would have little relevance in our study.\n",
    "2. There are sometimes several different people with the same name. For example, there is both Donald Trump the politician we all know and love but there's also another Donald Trump (a physician) in wikidata. Here we can assume that all quotes with speaker Donald Trump correspond to the politican but there are other cases. The name Tim Cahill is shared by an American Football player, an American Politician, a Screenwriter and more. Most quotes here come from the American football player but it is very complicated to identifiy which quotes corresponds to which Tim Cahill. Since, we have a vast amount of data we decided to manually handle a few cases which correspond to famous politicians such as Donald Trump and for other cases such as Tim Cahill we discard all the quotes related to that name.\n",
    "3. Several speakers such as Hillary Clinton and Donald Trump have been members of both the Democrats and Republicans in their lives. At first we discarded all such cases but since Trump and Clinton are famous politicans we manually attributed the party label to them corresponding to the party they are most well associated with (Democrats for Clinton and Republicans for Trump). We apply the same method for a few other politicians such as Michael Bloomberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting which political party does a quote's message lean towards\n",
    "\n",
    "Our first task is to create a model that can classify the political inclinations of a single quote.\n",
    "\n",
    "Since, our classification task strongly resembles that of NLP sentiment analysis\n",
    "we applied a corresponding methodology.\n",
    "\n",
    "Which can be summed up by the following steps by the following steps:\n",
    "1. Label data (done in part2)\n",
    "2. Clean quotations (augmented in part3)\n",
    "3. Vectorize quotations\n",
    "4. Train models and select optimal model for prediction\n",
    "\n",
    "Due to the complex nature of the task, which is to predict whether a single quote\n",
    "was said by a republican or democrat politician reaching a high accuracy is very\n",
    "difficult and so we had to optimize all the steps described above.\n",
    "\n",
    "Also, as noted in the course and on various online resources. It is sometimes\n",
    "better to have less data cleaning and text preprocessing in an\n",
    "NLP sentiment analysis tasks.\n",
    "\n",
    "We therfore had to find the optimal pipeline. This required finding the best \n",
    "combination of text preprocessor/cleaner, vectorizer and ML model. Given, that\n",
    "this isn't an ML class we tested a few computationally simple models\n",
    "(which can also be trained in a reasonable time frame) and focused rather on\n",
    "optimizing the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal level of preprocessing\n",
    "\n",
    "Our strategy in order to complete this task is to generate a dataset of quotes\n",
    "where each quote has with 5 different levels of text preprocessing, ranging \n",
    "from very light to very strong. Then we run cross validation with a few simple models \n",
    "(to keep execution time reasonable) and aggregate a few performance metrics to\n",
    "identify which level of preprocessing yielded the best performance with our model.\n",
    "\n",
    "Unlike for the final classification pipeline/model we perform all this only with \n",
    "quotes from 2020 since it is a reasonably sized snapshot of the data for this task.\n",
    "\n",
    "Each level of preprocessing was given a 1 letter name A,B,...,E.\n",
    "Here is a description of the 5 different levels of preprocessing:\n",
    "- A: Some trivial cleanup, removing digits and diacritics.\n",
    "- B: All steps in A + casefolding and removing punctuation.\n",
    "- C: All steps in B + removing stopwords.\n",
    "- D: All steps in C + stemming. Using the snowball stemmer.\n",
    "- E: All steps in C + lemmatization. Using nltk's WordNetLemmatizer.\n",
    "\n",
    "We perform the analysis in the cells below. We first prepare the data\n",
    "before running out tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396818, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>party_label</th>\n",
       "      <th>quotation_cleanA</th>\n",
       "      <th>quotation_cleanB</th>\n",
       "      <th>quotation_cleanC</th>\n",
       "      <th>quotation_cleanD</th>\n",
       "      <th>quotation_cleanE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-16-000088</td>\n",
       "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
       "      <td>Sue Myrick</td>\n",
       "      <td>2020-01-16 12:00:13</td>\n",
       "      <td>Q367796</td>\n",
       "      <td>R</td>\n",
       "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
       "      <td>department of homeland security was livid and ...</td>\n",
       "      <td>department homeland security livid strongly ur...</td>\n",
       "      <td>depart homeland secur livid strong urg agenda ...</td>\n",
       "      <td>department homeland security livid strongly ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-19-000276</td>\n",
       "      <td>[ These ] actions will allow households who ha...</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>2020-03-19 19:14:00</td>\n",
       "      <td>Q816459</td>\n",
       "      <td>R</td>\n",
       "      <td>[ These ] actions will allow households who ha...</td>\n",
       "      <td>these actions will allow households who have a...</td>\n",
       "      <td>actions allow households fha insured mortgage ...</td>\n",
       "      <td>action allow household fha insur mortgag meet ...</td>\n",
       "      <td>action allow household fha insured mortgage me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-22-009723</td>\n",
       "      <td>be pivotal in addressing financial frustrations</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>2020-01-22 21:07:39</td>\n",
       "      <td>Q816459</td>\n",
       "      <td>R</td>\n",
       "      <td>be pivotal in addressing financial frustrations</td>\n",
       "      <td>be pivotal in addressing financial frustrations</td>\n",
       "      <td>pivotal addressing financial frustrations</td>\n",
       "      <td>pivot address financi frustrat</td>\n",
       "      <td>pivotal addressing financial frustration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-04-110477</td>\n",
       "      <td>We're talking about `Do we want to continue th...</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>2020-02-04 23:02:36</td>\n",
       "      <td>Q816459</td>\n",
       "      <td>R</td>\n",
       "      <td>We're talking about `Do we want to continue th...</td>\n",
       "      <td>we re talking about do we want to continue the...</td>\n",
       "      <td>talking want continue lifestyle characterized ...</td>\n",
       "      <td>talk want continu lifestyl character american ...</td>\n",
       "      <td>talking want continue lifestyle characterized ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-28-051506</td>\n",
       "      <td>It's not just a matter of throwing more and mo...</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>2020-01-28 19:23:36</td>\n",
       "      <td>Q816459</td>\n",
       "      <td>R</td>\n",
       "      <td>It's not just a matter of throwing more and mo...</td>\n",
       "      <td>it s not just a matter of throwing more and mo...</td>\n",
       "      <td>matter throwing money vouchers services gettin...</td>\n",
       "      <td>matter throw money voucher servic get peopl sy...</td>\n",
       "      <td>matter throwing money voucher service getting ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2020-01-16-000088  [ Department of Homeland Security ] was livid ...   \n",
       "1  2020-03-19-000276  [ These ] actions will allow households who ha...   \n",
       "2  2020-01-22-009723    be pivotal in addressing financial frustrations   \n",
       "3  2020-02-04-110477  We're talking about `Do we want to continue th...   \n",
       "4  2020-01-28-051506  It's not just a matter of throwing more and mo...   \n",
       "\n",
       "      speaker                date       id party_label  \\\n",
       "0  Sue Myrick 2020-01-16 12:00:13  Q367796           R   \n",
       "1  Ben Carson 2020-03-19 19:14:00  Q816459           R   \n",
       "2  Ben Carson 2020-01-22 21:07:39  Q816459           R   \n",
       "3  Ben Carson 2020-02-04 23:02:36  Q816459           R   \n",
       "4  Ben Carson 2020-01-28 19:23:36  Q816459           R   \n",
       "\n",
       "                                    quotation_cleanA  \\\n",
       "0  [ Department of Homeland Security ] was livid ...   \n",
       "1  [ These ] actions will allow households who ha...   \n",
       "2    be pivotal in addressing financial frustrations   \n",
       "3  We're talking about `Do we want to continue th...   \n",
       "4  It's not just a matter of throwing more and mo...   \n",
       "\n",
       "                                    quotation_cleanB  \\\n",
       "0  department of homeland security was livid and ...   \n",
       "1  these actions will allow households who have a...   \n",
       "2    be pivotal in addressing financial frustrations   \n",
       "3  we re talking about do we want to continue the...   \n",
       "4  it s not just a matter of throwing more and mo...   \n",
       "\n",
       "                                    quotation_cleanC  \\\n",
       "0  department homeland security livid strongly ur...   \n",
       "1  actions allow households fha insured mortgage ...   \n",
       "2          pivotal addressing financial frustrations   \n",
       "3  talking want continue lifestyle characterized ...   \n",
       "4  matter throwing money vouchers services gettin...   \n",
       "\n",
       "                                    quotation_cleanD  \\\n",
       "0  depart homeland secur livid strong urg agenda ...   \n",
       "1  action allow household fha insur mortgag meet ...   \n",
       "2                     pivot address financi frustrat   \n",
       "3  talk want continu lifestyl character american ...   \n",
       "4  matter throw money voucher servic get peopl sy...   \n",
       "\n",
       "                                    quotation_cleanE  \n",
       "0  department homeland security livid strongly ur...  \n",
       "1  action allow household fha insured mortgage me...  \n",
       "2           pivotal addressing financial frustration  \n",
       "3  talking want continue lifestyle characterized ...  \n",
       "4  matter throwing money voucher service getting ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load preprocessed data containing all variants of text processing\n",
    "path = fixpath(QUOTES_2020_LABELED_CLEANED_VARIANTS)\n",
    "\n",
    "df_raw = pd.read_json(path, orient='records', lines=True)\n",
    "print(df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# Droping quotes of people in both parties (except most popular members who were labeled manually)\n",
    "df = df[df['party_label'] != 'RD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop very short quotes as a quote that is particularly short will give us\n",
    "little information and would most likely be irrelevant in the classification\n",
    "task. We just drop quotes shorter than 90% of all other quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(349675, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droping short quotes. Quotes shorter than 90% of all other quotes\n",
    "df = drop_short_quotes(df, threshold_quantile=0.1, quote_col_name='quotation_cleanE')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    198353\n",
       "R    151322\n",
       "Name: party_label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if dataset is balanced\n",
    "df.party_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is unbalanced. Since we have a lot of data we just downsample\n",
    "df = downsample(df, 'party_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    151322\n",
       "R    151322\n",
       "Name: party_label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the data is well balanced now\n",
    "df['party_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several different sized version of the data for convenience. Since some\n",
    "# models we test take long to train. Our final prediction for best level of\n",
    "# preprocessing will be done on the full data frame (~220k quotes from 2020)\n",
    "# that we generated above.\n",
    "\n",
    "df_micro = df.sample(1000)\n",
    "df_mini = df.sample(10000)\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "Now, we'll run cross validation on all combinations of files to find the best model and level of preprocessing. Here's a function for convenience and cleanliness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifer(df, pipeline, break_after_one_iter=False):\n",
    "    \"\"\"\n",
    "    Function to test different all version of preprocessed quotes with a given\n",
    "    classifer.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'quotation_cleanA',\n",
    "        'quotation_cleanB',\n",
    "        'quotation_cleanC',\n",
    "        'quotation_cleanD',\n",
    "        'quotation_cleanE',\n",
    "    ]\n",
    "\n",
    "    for col in cols:\n",
    "        \n",
    "        # Get quotation preprocessing variant\n",
    "        X = df[col].values\n",
    "\n",
    "        # Get label and convert to useful format\n",
    "        y = df['party_label'].values\n",
    "        y = convert_labels(y)\n",
    "\n",
    "        # Run cross validation with different metrics\n",
    "        # scoring=['accuracy', 'precision', 'recall', 'f1']\n",
    "        scoring=['accuracy', 'f1']\n",
    "        res = cross_validate(pipeline, X, y, scoring=scoring, cv=3)\n",
    "        res.pop('score_time')\n",
    "\n",
    "        # Print results\n",
    "        print(f'Col: {col}')\n",
    "        print_cross_validate_results(res)\n",
    "        \n",
    "        if break_after_one_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each level of preprocessed text we run cross validations with 3 different ML models. \n",
    "Multinomial Naive Bayes, LogisticRegression and Gradient Boosted Trees.\n",
    "\n",
    "We only use Tfidf vectorization but test different levels of N-gram expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 4.220\tstd: 0.112\n",
      "\ttest_accuracy        - \tavg: 0.691\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.695\tstd: 0.002\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 4.399\tstd: 0.373\n",
      "\ttest_accuracy        - \tavg: 0.691\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.695\tstd: 0.002\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 3.043\tstd: 0.216\n",
      "\ttest_accuracy        - \tavg: 0.689\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.692\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 2.657\tstd: 0.028\n",
      "\ttest_accuracy        - \tavg: 0.679\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.682\tstd: 0.002\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 2.740\tstd: 0.140\n",
      "\ttest_accuracy        - \tavg: 0.686\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.688\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 13.313\tstd: 0.758\n",
      "\ttest_accuracy        - \tavg: 0.692\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.692\tstd: 0.002\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 12.015\tstd: 0.806\n",
      "\ttest_accuracy        - \tavg: 0.692\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.692\tstd: 0.002\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 5.339\tstd: 0.540\n",
      "\ttest_accuracy        - \tavg: 0.689\tstd: 0.003\n",
      "\ttest_f1              - \tavg: 0.689\tstd: 0.003\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 5.875\tstd: 0.248\n",
      "\ttest_accuracy        - \tavg: 0.682\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.680\tstd: 0.002\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 6.857\tstd: 1.005\n",
      "\ttest_accuracy        - \tavg: 0.687\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.686\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('clf', LogisticRegression(max_iter=1000)),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 193.228\tstd: 5.168\n",
      "\ttest_accuracy        - \tavg: 0.632\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.619\tstd: 0.003\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 199.264\tstd: 14.585\n",
      "\ttest_accuracy        - \tavg: 0.632\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.623\tstd: 0.003\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 107.047\tstd: 0.712\n",
      "\ttest_accuracy        - \tavg: 0.628\tstd: 0.003\n",
      "\ttest_f1              - \tavg: 0.581\tstd: 0.004\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 104.179\tstd: 1.879\n",
      "\ttest_accuracy        - \tavg: 0.631\tstd: 0.003\n",
      "\ttest_f1              - \tavg: 0.596\tstd: 0.006\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 105.497\tstd: 1.417\n",
      "\ttest_accuracy        - \tavg: 0.630\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.588\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('clf', GradientBoostingClassifier(learning_rate=1)),\n",
    "        ])\n",
    "\n",
    "# Very long to run do sometime later\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 6.754\tstd: 0.082\n",
      "\ttest_accuracy        - \tavg: 0.696\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.697\tstd: 0.002\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 6.652\tstd: 0.049\n",
      "\ttest_accuracy        - \tavg: 0.696\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.697\tstd: 0.002\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 4.918\tstd: 0.310\n",
      "\ttest_accuracy        - \tavg: 0.693\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.693\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 4.893\tstd: 0.268\n",
      "\ttest_accuracy        - \tavg: 0.685\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.684\tstd: 0.002\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 4.830\tstd: 0.004\n",
      "\ttest_accuracy        - \tavg: 0.691\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.690\tstd: 0.003\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('clf', LinearSVC()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try using using unigrams and bigrams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 12.863\tstd: 0.103\n",
      "\ttest_accuracy        - \tavg: 0.733\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.740\tstd: 0.002\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 12.739\tstd: 0.025\n",
      "\ttest_accuracy        - \tavg: 0.733\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.740\tstd: 0.002\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 9.833\tstd: 0.147\n",
      "\ttest_accuracy        - \tavg: 0.744\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.747\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 8.791\tstd: 0.113\n",
      "\ttest_accuracy        - \tavg: 0.739\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.743\tstd: 0.001\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 9.661\tstd: 0.021\n",
      "\ttest_accuracy        - \tavg: 0.742\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.746\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 32.950\tstd: 4.012\n",
      "\ttest_accuracy        - \tavg: 0.723\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.724\tstd: 0.002\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 37.281\tstd: 4.540\n",
      "\ttest_accuracy        - \tavg: 0.723\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.724\tstd: 0.002\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 28.543\tstd: 7.263\n",
      "\ttest_accuracy        - \tavg: 0.727\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.728\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 23.331\tstd: 3.219\n",
      "\ttest_accuracy        - \tavg: 0.724\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.725\tstd: 0.001\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 31.883\tstd: 5.888\n",
      "\ttest_accuracy        - \tavg: 0.727\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.727\tstd: 0.001\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "            ('clf', LogisticRegression(max_iter=1000)),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 18.643\tstd: 0.121\n",
      "\ttest_accuracy        - \tavg: 0.740\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.740\tstd: 0.001\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 18.911\tstd: 0.357\n",
      "\ttest_accuracy        - \tavg: 0.740\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.740\tstd: 0.001\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 13.641\tstd: 0.110\n",
      "\ttest_accuracy        - \tavg: 0.747\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.748\tstd: 0.001\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 12.134\tstd: 0.333\n",
      "\ttest_accuracy        - \tavg: 0.743\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.744\tstd: 0.001\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 13.292\tstd: 0.474\n",
      "\ttest_accuracy        - \tavg: 0.746\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.747\tstd: 0.001\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "            ('clf', LinearSVC()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the best performing classifer is Multinomial Naive Bayes, its also\n",
    "the fastest one to train.\n",
    "\n",
    "How about adding trigrams too! We don't train all 3 models as training times\n",
    "get out of hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 30.872\tstd: 0.259\n",
      "\ttest_accuracy        - \tavg: 0.743\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.750\tstd: 0.001\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 30.110\tstd: 0.378\n",
      "\ttest_accuracy        - \tavg: 0.743\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.750\tstd: 0.001\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 20.760\tstd: 1.678\n",
      "\ttest_accuracy        - \tavg: 0.752\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.755\tstd: 0.002\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 18.049\tstd: 0.195\n",
      "\ttest_accuracy        - \tavg: 0.749\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.752\tstd: 0.001\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 18.768\tstd: 0.158\n",
      "\ttest_accuracy        - \tavg: 0.752\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.755\tstd: 0.001\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,3))),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 34.433\tstd: 0.513\n",
      "\ttest_accuracy        - \tavg: 0.750\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.752\tstd: 0.001\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 34.416\tstd: 0.420\n",
      "\ttest_accuracy        - \tavg: 0.750\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.752\tstd: 0.001\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 22.700\tstd: 0.272\n",
      "\ttest_accuracy        - \tavg: 0.754\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.755\tstd: 0.001\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 21.397\tstd: 0.053\n",
      "\ttest_accuracy        - \tavg: 0.752\tstd: 0.001\n",
      "\ttest_f1              - \tavg: 0.754\tstd: 0.001\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 22.074\tstd: 0.173\n",
      "\ttest_accuracy        - \tavg: 0.753\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.755\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,3))),\n",
    "            ('clf', LinearSVC()),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: quotation_cleanA\n",
      "\tfit_time             - \tavg: 32.895\tstd: 0.624\n",
      "\ttest_accuracy        - \tavg: 0.736\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.744\tstd: 0.002\n",
      "Col: quotation_cleanB\n",
      "\tfit_time             - \tavg: 33.253\tstd: 1.347\n",
      "\ttest_accuracy        - \tavg: 0.736\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.744\tstd: 0.002\n",
      "Col: quotation_cleanC\n",
      "\tfit_time             - \tavg: 21.559\tstd: 0.430\n",
      "\ttest_accuracy        - \tavg: 0.747\tstd: 0.003\n",
      "\ttest_f1              - \tavg: 0.750\tstd: 0.003\n",
      "Col: quotation_cleanD\n",
      "\tfit_time             - \tavg: 20.027\tstd: 0.142\n",
      "\ttest_accuracy        - \tavg: 0.745\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.748\tstd: 0.001\n",
      "Col: quotation_cleanE\n",
      "\tfit_time             - \tavg: 21.259\tstd: 0.317\n",
      "\ttest_accuracy        - \tavg: 0.747\tstd: 0.002\n",
      "\ttest_f1              - \tavg: 0.749\tstd: 0.002\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(ngram_range=(1,3))),\n",
    "            ('clf', MultinomialNB(alpha=1.8)),\n",
    "        ])\n",
    "\n",
    "test_classifer(df, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we achieve our best accuracy yet by using uni, bi and tri-grams. So we will use TfidfVectorizer(ngram_range=(1,3)).\n",
    "\n",
    "Here we achieve our best accuracy of 75% with both LinearSVC and MultinomialNB. Since the difference between the performance of both models is negligible we decided to use MultinomialNB from now on since it takes much less time to train. The same reasoning applies to the level of preprocessing we will use. We chose to use the most thorough level of cleaning (E) since it would reduce the complexity of our model (by reducing the size of the vectorizer's dictionary).\n",
    "\n",
    "Our predictions could likely be further improved by using a more advanced method for vectorizing the quotes such as word2vec or BERT but we decided to stick with a simpler less convoluted model and focus on further data analysis.\n",
    "\n",
    "Later, we will also aggregate our predictions to predict the political affiliation of a speaker based on all the quotes that are attributed to them and as such we can expect a better performance there!\n",
    "\n",
    "Now we will train out main model to be used throughout our project. Which we do in the [next notebook](part3_2-model_training.ipynb)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6809d3f7e90e77d8ec96c72c39a08b2522313d47c0f1d91518fdbbaf790b394"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ada': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
